{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c120dc70-182d-4950-b3b9-0e0c1079cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c31bd-515e-4dfc-ad29-d80299e5ad82",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde18f6a-7e00-4db6-ab20-73c4e640868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_illistrations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove Illustrations from the text\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove Illustrations\n",
    "    cleaned = re.sub(\n",
    "        r\"\\[Illustration(?::.*?\\]{1,2}|\\])\",\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Remove left over line\n",
    "    cleaned = re.sub(r\"\\n\\s*\\n\", \"\\n\", cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0049e14-60ec-439b-88a0-3de6db8ee388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gutenberg_header_footer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove the Project Gutenberg header and footer, and trim the text.\n",
    "    Only keeps text starting from the last occurrence of 'CHAPTER I' followed by a newline.\n",
    "    \"\"\"\n",
    "\n",
    "    start_marker = \"CHAPTER I\\n\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "    # Use the last occurrence of the chapter marker followed by a newline\n",
    "    start_idx = text.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        # Keep text starting at the final chapter marker,\n",
    "        # then remove its heading line\n",
    "        text = text[start_idx + len(start_marker):]\n",
    "\n",
    "    # Locate and remove the footer\n",
    "    end_idx = text.find(end_marker)\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a9b8ac7-5ca0-44a8-b2f7-ba279e2de56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_chapter(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a book into chapters, remove chapter titles,\n",
    "    and skip very short sections (like table of contents).\n",
    "    \"\"\"\n",
    "\n",
    "    chapters = []\n",
    "    chapter_marker = \"CHAPTER\"\n",
    "\n",
    "    # Split the text at each occurrence of the chapter marker\n",
    "    parts = text.split(chapter_marker)\n",
    "\n",
    "    for part in parts:\n",
    "        # Remove leading/trailing whitespace\n",
    "        chapter_text = part.strip()\n",
    "\n",
    "        # Skip very short sections (likely TOC or preface)\n",
    "        if len(chapter_text.split()) < 20:\n",
    "            continue\n",
    "\n",
    "        # Convert newlines to spaces\n",
    "        chapter_text = chapter_text.replace(\"\\n\", \" \")\n",
    "\n",
    "        # _word_ used for formatting somehow by project gutenberg\n",
    "        chapter_text = chapter_text.replace(\"_\", \" \")\n",
    "\n",
    "        chapters.append(chapter_text)\n",
    "\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dee0551-43d5-41b3-b592-e8f2db60dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(books: list[str]) -> list[str]:\n",
    "    Corpus = None\n",
    "    \n",
    "    for book in books:\n",
    "        book_no_images = remove_illistrations(book)\n",
    "        book_corpus = remove_gutenberg_header_footer(book_no_images)\n",
    "        book_chapters = split_by_chapter(book_corpus)\n",
    "\n",
    "        if Corpus is None:\n",
    "            Corpus = book_chapters\n",
    "        else:\n",
    "            Corpus += book_chapters\n",
    "\n",
    "    return Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68813914-b751-44d0-8307-d7cb2f508dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "with open('Books/Emma.txt', 'r') as emma:\n",
    "    books.append(emma.read())\n",
    "\n",
    "with open('Books/Pride_and_Prejudice_Jane_Austin.txt', 'r') as pride:\n",
    "    books.append(pride.read())\n",
    "\n",
    "with open('Books/Sense_and_Sensibility.txt', 'r') as sense:\n",
    "    books.append(sense.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "676d4f22-6e6a-4785-8e91-300f36754108",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = create_corpus(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb1515-6bcd-4a9e-8e42-24ed358b654a",
   "metadata": {},
   "source": [
    "# N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2356a94f-7380-4315-9f91-99230e704cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(corp):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a string of the cleaned text\n",
    "    \"\"\"    \n",
    "    STOPWORDS = set(stopwords.words(\"english\"))\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in corp if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # remove new lines\n",
    "    nopunc = nopunc.replace('\\n', '')\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word.lower() for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06084f5c-dbbd-45ac-85d9-42d65b0ff926",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = []\n",
    "for chapter in Corpus:\n",
    "    clean_corpus.append(text_process(chapter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "346446a9-abf0-4c22-a29b-0bada43f3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_ngrams(texts, n=2):\n",
    "    counts = Counter()\n",
    "    \n",
    "    for chapter in texts:\n",
    "        words = str(chapter).split()\n",
    "        for i in range(len(words) - n + 1):\n",
    "            ngram = tuple(words[i:i+n])\n",
    "            counts[ngram] += 1\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de131465-9eaa-4621-b54c-9568e4428be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('mr', 'knightley'), 233), (('mr', 'darcy'), 219), (('mrs', 'weston'), 210), (('mrs', 'jennings'), 198), (('every', 'thing'), 192), (('mr', 'elton'), 163), (('mrs', 'bennet'), 137), (('mr', 'weston'), 129), (('miss', 'woodhouse'), 129), (('young', 'man'), 129), (('mr', 'collins'), 121), (('frank', 'churchill'), 120), (('every', 'body'), 117), (('mrs', 'elton'), 113), (('dare', 'say'), 109), (('mrs', 'dashwood'), 109), (('great', 'deal'), 108), (('colonel', 'brandon'), 105), (('mr', 'woodhouse'), 103), (('sir', 'john'), 97)]\n"
     ]
    }
   ],
   "source": [
    "# Example: bigrams\n",
    "bigrams = word_ngrams(clean_corpus, n=2)\n",
    "print(bigrams.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "862b70ef-3fdb-42ee-b915-ee496cb08678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_lookup(ngram_counts, n):\n",
    "    lookup = defaultdict(list)\n",
    "\n",
    "    for ngram, count in ngram_counts.items():\n",
    "        key = ngram[:-1]      # first n-1 words\n",
    "        next_word = ngram[-1]\n",
    "\n",
    "        lookup[key] += [next_word] * count\n",
    "\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ad1099c-938d-4638-9203-fdec2ec3ae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(lookup, n=1, length=20):\n",
    "    start = random.choice(list(lookup.keys()))\n",
    "    sentence = list(start)\n",
    "\n",
    "    for _ in range(length):\n",
    "        key = tuple(sentence[-(n-1):])\n",
    "\n",
    "        if key not in lookup:\n",
    "            break\n",
    "\n",
    "        sentence.append(random.choice(lookup[key]))\n",
    "\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fa1b3d-d638-4087-89a8-575c1a743e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spend large fortune said mrs dashwood children rich without help must begin improvements house observed elinor difficulties soon vanish magnificent orders would travel family\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "trigrams = word_ngrams(clean_corpus, n=N)\n",
    "lookup = ngram_lookup(trigrams, n=N)\n",
    "\n",
    "print(generate_sentence(lookup, n=N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612dafee-3678-4519-b24a-90012ed7f21f",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Just did this for fun, found out that most of the algorthims that need it have it built in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d28efab-7e03-44a0-b709-e18038e12bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_idf_model  = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    ")\n",
    "tf_idf_vector = tr_idf_model.fit_transform(Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a197011-f71c-4d88-acf9-1374f00c6d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>26th</th>\n",
       "      <th>28th</th>\n",
       "      <th>abatement</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorred</th>\n",
       "      <th>abhorrence</th>\n",
       "      <th>abide</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>yorkshire</th>\n",
       "      <th>young</th>\n",
       "      <th>younge</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041932</td>\n",
       "      <td>0.036662</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017819</td>\n",
       "      <td>0.026559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.038396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows Ã— 6487 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     26th  28th  abatement     abbey  abhor  abhorred  abhorrence  abide  \\\n",
       "0     0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "1     0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "2     0.0   0.0   0.000000  0.033557    0.0       0.0    0.000000    0.0   \n",
       "3     0.0   0.0   0.000000  0.040157    0.0       0.0    0.000000    0.0   \n",
       "4     0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "..    ...   ...        ...       ...    ...       ...         ...    ...   \n",
       "160   0.0   0.0   0.038396  0.000000    0.0       0.0    0.028446    0.0   \n",
       "161   0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "162   0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "163   0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "164   0.0   0.0   0.000000  0.000000    0.0       0.0    0.000000    0.0   \n",
       "\n",
       "     abilities  ability  ...  york  yorkshire     young  younge  younger  \\\n",
       "0          0.0      0.0  ...   0.0   0.000000  0.021750     0.0      0.0   \n",
       "1          0.0      0.0  ...   0.0   0.041932  0.036662     0.0      0.0   \n",
       "2          0.0      0.0  ...   0.0   0.000000  0.059847     0.0      0.0   \n",
       "3          0.0      0.0  ...   0.0   0.000000  0.100264     0.0      0.0   \n",
       "4          0.0      0.0  ...   0.0   0.000000  0.020600     0.0      0.0   \n",
       "..         ...      ...  ...   ...        ...       ...     ...      ...   \n",
       "160        0.0      0.0  ...   0.0   0.000000  0.009299     0.0      0.0   \n",
       "161        0.0      0.0  ...   0.0   0.000000  0.029928     0.0      0.0   \n",
       "162        0.0      0.0  ...   0.0   0.000000  0.000000     0.0      0.0   \n",
       "163        0.0      0.0  ...   0.0   0.000000  0.000000     0.0      0.0   \n",
       "164        0.0      0.0  ...   0.0   0.000000  0.009541     0.0      0.0   \n",
       "\n",
       "     youngest     youth  youthful  zeal   zealous  \n",
       "0    0.019941  0.000000  0.000000   0.0  0.000000  \n",
       "1    0.000000  0.060812  0.000000   0.0  0.000000  \n",
       "2    0.000000  0.089343  0.000000   0.0  0.000000  \n",
       "3    0.000000  0.017819  0.026559   0.0  0.000000  \n",
       "4    0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "..        ...       ...       ...   ...       ...  \n",
       "160  0.000000  0.023137  0.000000   0.0  0.038396  \n",
       "161  0.027438  0.000000  0.000000   0.0  0.000000  \n",
       "162  0.043978  0.000000  0.000000   0.0  0.000000  \n",
       "163  0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "164  0.000000  0.000000  0.000000   0.0  0.000000  \n",
       "\n",
       "[165 rows x 6487 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_set = tr_idf_model.get_feature_names_out()\n",
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "\n",
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa93432-d57d-47de-b2de-6b8f30ee7b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
