{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4d1f8b-8f4a-4d15-8320-e19bd5963a23",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72397dc4-c5e0-4ecf-92ad-5f2265417404",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Before applying any NLP models, the raw text data must be cleaned and standardized.  \n",
    "This preprocessing step removes non-linguistic artifacts (such as illustrations),\n",
    "normalizes formatting, and prepares the text so that downstream models analyze\n",
    "meaningful language patterns rather than noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d424ed3b-8cbe-4008-840a-8ea405d0e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca73cf4-6a11-4982-9b65-49b272767d79",
   "metadata": {},
   "source": [
    "### Removing Illustrations and Non-Text Elements\n",
    "\n",
    "Luckily Project Gutenberg texts contain placeholders such as [Illustration], as well as headers and footers about copyright and other placeholders. Luckily they make these very easy to pick out.\n",
    "\n",
    "These artifacts can distort word frequency counts and topic modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e294f64-ae4a-4ab4-83b1-a4efc3c722ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_illistrations(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove Illustrations from the text using regex\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove Illustrations\n",
    "    cleaned = re.sub(\n",
    "        r\"\\[Illustration(?::.*?\\]{1,2}|\\])\", # Searches for [Illustration] or [Illustration: description] or [[some text]]\n",
    "        \"\", # Replaces with blank string\n",
    "        text,\n",
    "        flags=re.DOTALL # catches this pattern if it spans multiple lines\n",
    "    )\n",
    "\n",
    "    # Remove left over line\n",
    "    cleaned = re.sub(r\"\\n\\s*\\n\", \"\\n\", cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8665b19-80ee-4786-aa68-9f0e01dae073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gutenberg_header_footer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove the Project Gutenberg header and footer, and trim the text.\n",
    "    Only keeps text starting from the last occurrence of 'CHAPTER I' followed by a newline.\n",
    "    \"\"\"\n",
    "\n",
    "    start_marker = \"CHAPTER I\\n\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "    # Use the last occurrence of the chapter marker followed by a newline\n",
    "    start_idx = text.find(start_marker)\n",
    "    if start_idx != -1:\n",
    "        # Keep text starting at the final chapter marker,\n",
    "        # then remove its heading line\n",
    "        text = text[start_idx + len(start_marker):]\n",
    "\n",
    "    # Locate and remove the footer\n",
    "    end_idx = text.find(end_marker)\n",
    "    if end_idx != -1:\n",
    "        text = text[:end_idx]\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec32ff6-8185-4c3b-820e-a941187d3d12",
   "metadata": {},
   "source": [
    "### Corpus Construction\n",
    "\n",
    "After cleaning individual texts, all books are split into chapters and combined into a single corpus.\n",
    "\n",
    "The corpus returned here is a list of cleaned text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e29b1c-326c-4cf4-aef6-e9ea961ca4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_chapter(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a book into chapters, remove chapter titles,\n",
    "    and skip very short sections (like table of contents).\n",
    "    \"\"\"\n",
    "\n",
    "    chapters = []\n",
    "    chapter_marker = \"CHAPTER\"\n",
    "\n",
    "    # Split the text at each occurrence of the chapter marker\n",
    "    parts = text.split(chapter_marker)\n",
    "\n",
    "    for part in parts:\n",
    "        # Remove leading/trailing whitespace\n",
    "        chapter_text = part.strip()\n",
    "\n",
    "        # Skip very short sections\n",
    "        if len(chapter_text.split()) < 20:\n",
    "            continue\n",
    "\n",
    "        # Convert newlines to spaces\n",
    "        chapter_text = chapter_text.replace(\"\\n\", \" \")\n",
    "\n",
    "        # remove _word_ which is used for formatting by project gutenberg\n",
    "        chapter_text = chapter_text.replace(\"_\", \" \")\n",
    "\n",
    "        chapters.append(chapter_text)\n",
    "\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faedabbf-3934-4c7c-95ff-c2176629be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(books: list[str]) -> list[str]:\n",
    "    Corpus = None\n",
    "    \n",
    "    for book in books:\n",
    "        book_no_images = remove_illistrations(book)\n",
    "        book_corpus = remove_gutenberg_header_footer(book_no_images)\n",
    "        book_chapters = split_by_chapter(book_corpus)\n",
    "\n",
    "        if Corpus is None: \n",
    "            Corpus = book_chapters\n",
    "        else:\n",
    "            Corpus += book_chapters\n",
    "\n",
    "    return Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b62065-3d0f-4b72-9474-3cbe78a4365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reads the contents of the three book files and appends them to a list\n",
    "\"\"\"\n",
    "def load_books(files: list) -> list[str]:\n",
    "    books = []\n",
    "    for file in files:\n",
    "        with open(f'Books/{file}', 'r') as contents:\n",
    "            books.append(contents.read())\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7eb329b0-9082-438d-b4bc-ea25ec7ea81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the three books, clean them and split them into chapters\n",
    "books = load_books([\"Emma.txt\",\"Pride_and_Prejudice_Jane_Austin.txt\",\"Sense_and_Sensibility.txt\"])\n",
    "\n",
    "Corpus = create_corpus(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a7c29-55cd-4f6a-83a6-c0528c64e625",
   "metadata": {},
   "source": [
    "### Final Preprocessed Corpus\n",
    "\n",
    "At this stage, all texts have been cleaned and combined into a single corpus.\n",
    "This corpus serves as the input for topic modeling and other NLP techniques\n",
    "used later in the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72e06c-aea6-467c-a44e-3454476e731a",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4fa08f-453b-47ea-bad9-e6bf1c38b5ac",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation is an unsupervised topic modeling technique that\n",
    "identifies latent themes in a collection of documents. Rather than understanding\n",
    "language semantically, LDA relies on word co-occurrence patterns to infer topics\n",
    "as probability distributions over words.\n",
    "\n",
    "This model is used here as a baseline NLP approach to compare against more\n",
    "advanced language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9031d0f9-04c9-4d48-b332-cbec6e2772b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c93bcaa-db48-422e-832b-a3dc3255d17c",
   "metadata": {},
   "source": [
    "### Text Vectorization\n",
    "\n",
    "LDA operates on numerical representations of text rather than raw strings.\n",
    "The CountVectorizer converts the corpus into a document-term matrix, where\n",
    "each entry represents the frequency of a word in a document.\n",
    "\n",
    "This bag-of-words approach ignores word order and context, which is a key\n",
    "limitation explored later in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0452d719-4e60-4f5c-8b89-0e7c1cb00392",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(Corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c775d-5ee4-4116-bf66-fc2ad95d440d",
   "metadata": {},
   "source": [
    "### Training the LDA Model\n",
    "\n",
    "The LDA model is trained on the document-term matrix to identify a fixed number\n",
    "of topics. Each topic is represented as a probability distribution over words,\n",
    "and each document is represented as a mixture of topics.\n",
    "\n",
    "The number of topics is chosen arbitrarily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f908003-8a11-4acd-8c48-01084839efe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.light {\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: black;\n",
       "  --sklearn-color-background: white;\n",
       "  --sklearn-color-border-box: black;\n",
       "  --sklearn-color-icon: #696969;\n",
       "}\n",
       "\n",
       "#sk-container-id-1.dark {\n",
       "  --sklearn-color-text-on-default-background: white;\n",
       "  --sklearn-color-background: #111;\n",
       "  --sklearn-color-border-box: white;\n",
       "  --sklearn-color-icon: #878787;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: center;\n",
       "  justify-content: center;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-0);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table {\n",
       "    font-family: monospace;\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table summary::marker {\n",
       "    font-size: 0.7rem;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "    margin-top: 0;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       "/*\n",
       "    `table td`is set in notebook with right text-align.\n",
       "    We need to overwrite it.\n",
       "*/\n",
       ".estimator-table table td.param {\n",
       "    text-align: left;\n",
       "    position: relative;\n",
       "    padding: 0;\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td.value {\n",
       "    color:rgb(255, 94, 0);\n",
       "    background-color: transparent;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left !important;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "/*\n",
       "    Styles for parameter documentation links\n",
       "    We need styling for visited so jupyter doesn't overwrite it\n",
       "*/\n",
       "a.param-doc-link,\n",
       "a.param-doc-link:link,\n",
       "a.param-doc-link:visited {\n",
       "    text-decoration: underline dashed;\n",
       "    text-underline-offset: .3em;\n",
       "    color: inherit;\n",
       "    display: block;\n",
       "    padding: .5em;\n",
       "}\n",
       "\n",
       "/* \"hack\" to make the entire area of the cell containing the link clickable */\n",
       "a.param-doc-link::before {\n",
       "    position: absolute;\n",
       "    content: \"\";\n",
       "    inset: 0;\n",
       "}\n",
       "\n",
       ".param-doc-description {\n",
       "    display: none;\n",
       "    position: absolute;\n",
       "    z-index: 9999;\n",
       "    left: 0;\n",
       "    padding: .5ex;\n",
       "    margin-left: 1.5em;\n",
       "    color: var(--sklearn-color-text);\n",
       "    box-shadow: .3em .3em .4em #999;\n",
       "    width: max-content;\n",
       "    text-align: left;\n",
       "    max-height: 10em;\n",
       "    overflow-y: auto;\n",
       "\n",
       "    /* unfitted */\n",
       "    background: var(--sklearn-color-unfitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       "/* Fitted state for parameter tooltips */\n",
       ".fitted .param-doc-description {\n",
       "    /* fitted */\n",
       "    background: var(--sklearn-color-fitted-level-0);\n",
       "    border: thin solid var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".param-doc-link:hover .param-doc-description {\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LatentDirichletAllocation</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\">?<span>Documentation for LatentDirichletAllocation</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_components',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=n_components,-int%2C%20default%3D10\">\n",
       "            n_components\n",
       "            <span class=\"param-doc-description\">n_components: int, default=10<br><br>Number of topics.<br><br>.. versionchanged:: 0.19<br>    ``n_topics`` was renamed to ``n_components``</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('doc_topic_prior',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=doc_topic_prior,-float%2C%20default%3DNone\">\n",
       "            doc_topic_prior\n",
       "            <span class=\"param-doc-description\">doc_topic_prior: float, default=None<br><br>Prior of document topic distribution `theta`. If the value is None,<br>defaults to `1 / n_components`.<br>In [1]_, this is called `alpha`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('topic_word_prior',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=topic_word_prior,-float%2C%20default%3DNone\">\n",
       "            topic_word_prior\n",
       "            <span class=\"param-doc-description\">topic_word_prior: float, default=None<br><br>Prior of topic word distribution `beta`. If the value is None, defaults<br>to `1 / n_components`.<br>In [1]_, this is called `eta`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_method',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=learning_method,-%7B%27batch%27%2C%20%27online%27%7D%2C%20default%3D%27batch%27\">\n",
       "            learning_method\n",
       "            <span class=\"param-doc-description\">learning_method: {'batch', 'online'}, default='batch'<br><br>Method used to update `_component`. Only used in :meth:`fit` method.<br>In general, if the data size is large, the online update will be much<br>faster than the batch update.<br><br>Valid options:<br><br>- 'batch': Batch variational Bayes method. Use all training data in each EM<br>  update. Old `components_` will be overwritten in each iteration.<br>- 'online': Online variational Bayes method. In each EM update, use mini-batch<br>  of training data to update the ``components_`` variable incrementally. The<br>  learning rate is controlled by the ``learning_decay`` and the<br>  ``learning_offset`` parameters.<br><br>.. versionchanged:: 0.20<br>    The default learning method is now ``\"batch\"``.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">&#x27;batch&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_decay',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=learning_decay,-float%2C%20default%3D0.7\">\n",
       "            learning_decay\n",
       "            <span class=\"param-doc-description\">learning_decay: float, default=0.7<br><br>It is a parameter that control learning rate in the online learning<br>method. The value should be set between (0.5, 1.0] to guarantee<br>asymptotic convergence. When the value is 0.0 and batch_size is<br>``n_samples``, the update method is same as batch learning. In the<br>literature, this is called kappa.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.7</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('learning_offset',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=learning_offset,-float%2C%20default%3D10.0\">\n",
       "            learning_offset\n",
       "            <span class=\"param-doc-description\">learning_offset: float, default=10.0<br><br>A (positive) parameter that downweights early iterations in online<br>learning.  It should be greater than 1.0. In the literature, this is<br>called tau_0.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">10.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=max_iter,-int%2C%20default%3D10\">\n",
       "            max_iter\n",
       "            <span class=\"param-doc-description\">max_iter: int, default=10<br><br>The maximum number of passes over the training data (aka epochs).<br>It only impacts the behavior in the :meth:`fit` method, and not the<br>:meth:`partial_fit` method.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('batch_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=batch_size,-int%2C%20default%3D128\">\n",
       "            batch_size\n",
       "            <span class=\"param-doc-description\">batch_size: int, default=128<br><br>Number of documents to use in each EM iteration. Only used in online<br>learning.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">128</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('evaluate_every',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=evaluate_every,-int%2C%20default%3D-1\">\n",
       "            evaluate_every\n",
       "            <span class=\"param-doc-description\">evaluate_every: int, default=-1<br><br>How often to evaluate perplexity. Only used in `fit` method.<br>set it to 0 or negative number to not evaluate perplexity in<br>training at all. Evaluating perplexity can help you check convergence<br>in training process, but it will also increase total training time.<br>Evaluating perplexity in every iteration might increase training time<br>up to two-fold.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('total_samples',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=total_samples,-int%2C%20default%3D1e6\">\n",
       "            total_samples\n",
       "            <span class=\"param-doc-description\">total_samples: int, default=1e6<br><br>Total number of documents. Only used in the :meth:`partial_fit` method.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">1000000.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('perp_tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=perp_tol,-float%2C%20default%3D1e-1\">\n",
       "            perp_tol\n",
       "            <span class=\"param-doc-description\">perp_tol: float, default=1e-1<br><br>Perplexity tolerance. Only used when ``evaluate_every`` is greater than 0.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('mean_change_tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=mean_change_tol,-float%2C%20default%3D1e-3\">\n",
       "            mean_change_tol\n",
       "            <span class=\"param-doc-description\">mean_change_tol: float, default=1e-3<br><br>Stopping tolerance for updating document topic distribution in E-step.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_doc_update_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=max_doc_update_iter,-int%2C%20default%3D100\">\n",
       "            max_doc_update_iter\n",
       "            <span class=\"param-doc-description\">max_doc_update_iter: int, default=100<br><br>Max number of iterations for updating document topic distribution in<br>the E-step.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n",
       "            n_jobs\n",
       "            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>The number of jobs to use in the E-step.<br>``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.<br>``-1`` means using all processors. See :term:`Glossary <n_jobs>`<br>for more details.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=verbose,-int%2C%20default%3D0\">\n",
       "            verbose\n",
       "            <span class=\"param-doc-description\">verbose: int, default=0<br><br>Verbosity level.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">\n",
       "        <a class=\"param-doc-link\"\n",
       "            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html#:~:text=random_state,-int%2C%20RandomState%20instance%20or%20None%2C%20default%3DNone\">\n",
       "            random_state\n",
       "            <span class=\"param-doc-description\">random_state: int, RandomState instance or None, default=None<br><br>Pass an int for reproducible results across multiple function calls.<br>See :term:`Glossary <random_state>`.</span>\n",
       "        </a>\n",
       "    </td>\n",
       "            <td class=\"value\">42</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling\n",
       "        .textContent.trim().split(' ')[0];\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "\n",
       "\n",
       "/**\n",
       " * Adapted from Skrub\n",
       " * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n",
       " * @returns \"light\" or \"dark\"\n",
       " */\n",
       "function detectTheme(element) {\n",
       "    const body = document.querySelector('body');\n",
       "\n",
       "    // Check VSCode theme\n",
       "    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n",
       "    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n",
       "\n",
       "    if (themeKindAttr && themeNameAttr) {\n",
       "        const themeKind = themeKindAttr.toLowerCase();\n",
       "        const themeName = themeNameAttr.toLowerCase();\n",
       "\n",
       "        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n",
       "            return \"dark\";\n",
       "        }\n",
       "        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n",
       "            return \"light\";\n",
       "        }\n",
       "    }\n",
       "\n",
       "    // Check Jupyter theme\n",
       "    if (body.getAttribute('data-jp-theme-light') === 'false') {\n",
       "        return 'dark';\n",
       "    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n",
       "        return 'light';\n",
       "    }\n",
       "\n",
       "    // Guess based on a parent element's color\n",
       "    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n",
       "    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n",
       "    if (match) {\n",
       "        const [r, g, b] = [\n",
       "            parseFloat(match[1]),\n",
       "            parseFloat(match[2]),\n",
       "            parseFloat(match[3])\n",
       "        ];\n",
       "\n",
       "        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n",
       "        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n",
       "\n",
       "        if (luma > 180) {\n",
       "            // If the text is very bright we have a dark theme\n",
       "            return 'dark';\n",
       "        }\n",
       "        if (luma < 75) {\n",
       "            // If the text is very dark we have a light theme\n",
       "            return 'light';\n",
       "        }\n",
       "        // Otherwise fall back to the next heuristic.\n",
       "    }\n",
       "\n",
       "    // Fallback to system preference\n",
       "    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n",
       "}\n",
       "\n",
       "\n",
       "function forceTheme(elementId) {\n",
       "    const estimatorElement = document.querySelector(`#${elementId}`);\n",
       "    if (estimatorElement === null) {\n",
       "        console.error(`Element with id ${elementId} not found.`);\n",
       "    } else {\n",
       "        const theme = detectTheme(estimatorElement);\n",
       "        estimatorElement.classList.add(theme);\n",
       "    }\n",
       "}\n",
       "\n",
       "forceTheme('sk-container-id-1');</script></body>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,         # experiment with 5–30\n",
    "    learning_method=\"batch\", # stable & reproducible\n",
    "    random_state=42,\n",
    ")\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a170a026-1fe3-4681-9a7a-64e021ebce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, vectorizer, n_words=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic #{idx}\")\n",
    "        print(\"  \" + \" \".join(words[i] for i in topic.argsort()[-n_words:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ff41e44-ad87-4786-82a8-586adcf3e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\n",
      "  woman heart affection marianne ferrars accomplished brother robert miss edward\n",
      "Topic #1\n",
      "  miss wickham did jane bingley said bennet darcy elizabeth mr\n",
      "Topic #2\n",
      "  heart day mr think mother did said sister elinor marianne\n",
      "Topic #3\n",
      "  think good weston said elton knightley miss harriet emma mr\n",
      "Topic #4\n",
      "  bennet lady bingley colonel room mr miss elizabeth darcy said\n",
      "Topic #5\n",
      "  father said miss thing john did mr good think dear\n",
      "Topic #6\n",
      "  did woodhouse thing think know weston said miss emma mr\n",
      "Topic #7\n",
      "  did sister know miss jennings lucy edward said marianne elinor\n",
      "Topic #8\n",
      "  campbell cole dixon emma bates thing mr fairfax miss jane\n",
      "Topic #9\n",
      "  did shall know dear said sister dashwood willoughby elinor marianne\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89703cfc-a85c-4797-a03d-b1929f4111ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 0: dominant topic = 3\n",
      "Chapter 1: dominant topic = 3\n",
      "Chapter 2: dominant topic = 8\n",
      "Chapter 3: dominant topic = 3\n",
      "Chapter 4: dominant topic = 6\n",
      "Chapter 5: dominant topic = 3\n",
      "Chapter 6: dominant topic = 3\n",
      "Chapter 7: dominant topic = 3\n",
      "Chapter 8: dominant topic = 5\n",
      "Chapter 9: dominant topic = 5\n",
      "Chapter 10: dominant topic = 3\n",
      "Chapter 11: dominant topic = 5\n",
      "Chapter 12: dominant topic = 3\n",
      "Chapter 13: dominant topic = 3\n",
      "Chapter 14: dominant topic = 3\n",
      "Chapter 15: dominant topic = 3\n",
      "Chapter 16: dominant topic = 3\n",
      "Chapter 17: dominant topic = 3\n",
      "Chapter 18: dominant topic = 8\n",
      "Chapter 19: dominant topic = 8\n",
      "Chapter 20: dominant topic = 6\n",
      "Chapter 21: dominant topic = 3\n",
      "Chapter 22: dominant topic = 6\n",
      "Chapter 23: dominant topic = 3\n",
      "Chapter 24: dominant topic = 3\n",
      "Chapter 25: dominant topic = 3\n",
      "Chapter 26: dominant topic = 6\n",
      "Chapter 27: dominant topic = 6\n",
      "Chapter 28: dominant topic = 6\n",
      "Chapter 29: dominant topic = 3\n",
      "Chapter 30: dominant topic = 3\n",
      "Chapter 31: dominant topic = 3\n",
      "Chapter 32: dominant topic = 3\n",
      "Chapter 33: dominant topic = 6\n",
      "Chapter 34: dominant topic = 5\n",
      "Chapter 35: dominant topic = 3\n",
      "Chapter 36: dominant topic = 3\n",
      "Chapter 37: dominant topic = 3\n",
      "Chapter 38: dominant topic = 3\n",
      "Chapter 39: dominant topic = 3\n",
      "Chapter 40: dominant topic = 3\n",
      "Chapter 41: dominant topic = 3\n",
      "Chapter 42: dominant topic = 3\n",
      "Chapter 43: dominant topic = 8\n",
      "Chapter 44: dominant topic = 3\n",
      "Chapter 45: dominant topic = 3\n",
      "Chapter 46: dominant topic = 3\n",
      "Chapter 47: dominant topic = 3\n",
      "Chapter 48: dominant topic = 3\n",
      "Chapter 49: dominant topic = 3\n",
      "Chapter 50: dominant topic = 3\n",
      "Chapter 51: dominant topic = 6\n",
      "Chapter 52: dominant topic = 3\n",
      "Chapter 53: dominant topic = 3\n",
      "Chapter 54: dominant topic = 3\n",
      "Chapter 55: dominant topic = 1\n",
      "Chapter 56: dominant topic = 1\n",
      "Chapter 57: dominant topic = 1\n",
      "Chapter 58: dominant topic = 1\n",
      "Chapter 59: dominant topic = 1\n",
      "Chapter 60: dominant topic = 1\n",
      "Chapter 61: dominant topic = 1\n",
      "Chapter 62: dominant topic = 1\n",
      "Chapter 63: dominant topic = 1\n",
      "Chapter 64: dominant topic = 1\n",
      "Chapter 65: dominant topic = 4\n",
      "Chapter 66: dominant topic = 1\n",
      "Chapter 67: dominant topic = 1\n",
      "Chapter 68: dominant topic = 1\n",
      "Chapter 69: dominant topic = 1\n",
      "Chapter 70: dominant topic = 1\n",
      "Chapter 71: dominant topic = 1\n",
      "Chapter 72: dominant topic = 1\n",
      "Chapter 73: dominant topic = 1\n",
      "Chapter 74: dominant topic = 1\n",
      "Chapter 75: dominant topic = 1\n",
      "Chapter 76: dominant topic = 1\n",
      "Chapter 77: dominant topic = 1\n",
      "Chapter 78: dominant topic = 1\n",
      "Chapter 79: dominant topic = 1\n",
      "Chapter 80: dominant topic = 1\n",
      "Chapter 81: dominant topic = 1\n",
      "Chapter 82: dominant topic = 1\n",
      "Chapter 83: dominant topic = 1\n",
      "Chapter 84: dominant topic = 1\n",
      "Chapter 85: dominant topic = 4\n",
      "Chapter 86: dominant topic = 1\n",
      "Chapter 87: dominant topic = 1\n",
      "Chapter 88: dominant topic = 2\n",
      "Chapter 89: dominant topic = 2\n",
      "Chapter 90: dominant topic = 1\n",
      "Chapter 91: dominant topic = 1\n",
      "Chapter 92: dominant topic = 1\n",
      "Chapter 93: dominant topic = 1\n",
      "Chapter 94: dominant topic = 1\n",
      "Chapter 95: dominant topic = 1\n",
      "Chapter 96: dominant topic = 1\n",
      "Chapter 97: dominant topic = 3\n",
      "Chapter 98: dominant topic = 1\n",
      "Chapter 99: dominant topic = 1\n",
      "Chapter 100: dominant topic = 1\n",
      "Chapter 101: dominant topic = 1\n",
      "Chapter 102: dominant topic = 1\n",
      "Chapter 103: dominant topic = 1\n",
      "Chapter 104: dominant topic = 1\n",
      "Chapter 105: dominant topic = 1\n",
      "Chapter 106: dominant topic = 1\n",
      "Chapter 107: dominant topic = 1\n",
      "Chapter 108: dominant topic = 1\n",
      "Chapter 109: dominant topic = 1\n",
      "Chapter 110: dominant topic = 1\n",
      "Chapter 111: dominant topic = 2\n",
      "Chapter 112: dominant topic = 1\n",
      "Chapter 113: dominant topic = 1\n",
      "Chapter 114: dominant topic = 1\n",
      "Chapter 115: dominant topic = 5\n",
      "Chapter 116: dominant topic = 5\n",
      "Chapter 117: dominant topic = 9\n",
      "Chapter 118: dominant topic = 2\n",
      "Chapter 119: dominant topic = 9\n",
      "Chapter 120: dominant topic = 2\n",
      "Chapter 121: dominant topic = 2\n",
      "Chapter 122: dominant topic = 2\n",
      "Chapter 123: dominant topic = 5\n",
      "Chapter 124: dominant topic = 2\n",
      "Chapter 125: dominant topic = 2\n",
      "Chapter 126: dominant topic = 7\n",
      "Chapter 127: dominant topic = 7\n",
      "Chapter 128: dominant topic = 2\n",
      "Chapter 129: dominant topic = 9\n",
      "Chapter 130: dominant topic = 9\n",
      "Chapter 131: dominant topic = 7\n",
      "Chapter 132: dominant topic = 7\n",
      "Chapter 133: dominant topic = 2\n",
      "Chapter 134: dominant topic = 7\n",
      "Chapter 135: dominant topic = 2\n",
      "Chapter 136: dominant topic = 7\n",
      "Chapter 137: dominant topic = 2\n",
      "Chapter 138: dominant topic = 2\n",
      "Chapter 139: dominant topic = 7\n",
      "Chapter 140: dominant topic = 7\n",
      "Chapter 141: dominant topic = 7\n",
      "Chapter 142: dominant topic = 7\n",
      "Chapter 143: dominant topic = 9\n",
      "Chapter 144: dominant topic = 7\n",
      "Chapter 145: dominant topic = 2\n",
      "Chapter 146: dominant topic = 7\n",
      "Chapter 147: dominant topic = 5\n",
      "Chapter 148: dominant topic = 7\n",
      "Chapter 149: dominant topic = 7\n",
      "Chapter 150: dominant topic = 7\n",
      "Chapter 151: dominant topic = 7\n",
      "Chapter 152: dominant topic = 7\n",
      "Chapter 153: dominant topic = 7\n",
      "Chapter 154: dominant topic = 7\n",
      "Chapter 155: dominant topic = 7\n",
      "Chapter 156: dominant topic = 2\n",
      "Chapter 157: dominant topic = 2\n",
      "Chapter 158: dominant topic = 2\n",
      "Chapter 159: dominant topic = 2\n",
      "Chapter 160: dominant topic = 2\n",
      "Chapter 161: dominant topic = 7\n",
      "Chapter 162: dominant topic = 7\n",
      "Chapter 163: dominant topic = 7\n",
      "Chapter 164: dominant topic = 0\n"
     ]
    }
   ],
   "source": [
    "chapter_topic_distrib = lda.transform(X)\n",
    "\n",
    "for i, distrib in enumerate(chapter_topic_distrib):\n",
    "    print(f\"Chapter {i}: dominant topic = {distrib.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978db8a-db5b-41b7-9b4d-4f3546aedbb7",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "While LDA is effective at identifying word co-occurrence patterns, the resulting\n",
    "topics often lack semantic coherence. Many topics appear as loosely related\n",
    "collections of words rather than meaningful summaries.\n",
    "\n",
    "This limitation motivates comparison with large language models, which capture\n",
    "context, syntax, and semantic relationships beyond simple frequency statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b61b2-afc1-4132-ba1b-9a91347a081e",
   "metadata": {},
   "source": [
    "# TextRank\n",
    "\n",
    "<https://medium.com/@yassineerraji/understanding-textrank-a-deep-dive-into-graph-based-text-summarization-and-keyword-extraction-905d1fb5d266>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdca7f-dbfd-44f7-b365-c4824b18a6f6",
   "metadata": {},
   "source": [
    "TextRank gives a way to summarize documents without any supervision. It helps show how much structure you can get out of text using only relationships between sentences.\n",
    "\n",
    "How it works conceptually:\n",
    "\n",
    "1. Each sentence = a node\n",
    "\n",
    "2. Sentences are connected if they share words\n",
    "\n",
    "3. Sentences that connect to many others score higher\n",
    "\n",
    "4. Top-scoring sentences become the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0386ccb6-956f-42f3-bbda-71c58db24091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf8b00f9-b449-4a52-b759-3e0b99fa061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytextrank.base.BaseTextRankFactory at 0x1629701a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0baac7d-7ed2-43ba-9b52-4df6b050c9ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document Size: 17790 \n",
      "\n",
      "Phrase: #1\n",
      "\n",
      "I am sure she will be an excellent servant; and it will be a great comfort to poor Miss Taylor to have somebody about her that she is used to see. \n",
      "\n",
      "------------------------------------------------------------------ \n",
      "\n",
      "Phrase: #2\n",
      "\n",
      "“Poor Mr. and Miss Woodhouse, if you please; but I cannot possibly say ‘poor Miss Taylor.’ \n",
      "\n",
      "------------------------------------------------------------------ \n",
      "\n",
      "Phrase: #3\n",
      "\n",
      "“But, Mr. Knightley, she is really very sorry to lose poor Miss Taylor, and I am sure she  will  miss her more than she thinks for.” \n",
      "\n",
      "------------------------------------------------------------------ \n",
      "\n",
      "Phrase: #4\n",
      "\n",
      "Sixteen years had Miss Taylor been in Mr. Woodhouse’s family, less as a governess than a friend, very fond of both daughters, but particularly of Emma. \n",
      "\n",
      "------------------------------------------------------------------ \n",
      "\n",
      "Phrase: #5\n",
      "\n",
      "Even before Miss Taylor had ceased to hold the nominal office of governess, the mildness of her temper had hardly allowed her to impose any restraint; and the shadow of authority being now long passed away, they had been living together as friend and friend very mutually attached, and Emma doing just what she liked; highly esteeming Miss Taylor’s judgment, but directed chiefly by her own. \n",
      "\n",
      "------------------------------------------------------------------ \n",
      "\n",
      "Total Summary Length: 914 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chapter_num = 0\n",
    "\n",
    "# Print size of the original document\n",
    "print('Original Document Size:', len(Corpus[chapter_num]), '\\n')\n",
    "\n",
    "# Process the chapter with the spacy pipeline\n",
    "doc = nlp(Corpus[chapter_num])\n",
    "\n",
    "final_summary = None\n",
    "phrase_count = 1\n",
    "\n",
    "# Generate summary using TextRank\n",
    "for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=5):\n",
    "    print(f\"Phrase: #{phrase_count}\\n\")\n",
    "    print(sent, '\\n')\n",
    "    print('------------------------------------------------------------------', '\\n')\n",
    "    \n",
    "    # Build the final summary string\n",
    "    if final_summary is None:\n",
    "        final_summary = str(sent)\n",
    "    else:\n",
    "        final_summary += \" \" + str(sent)\n",
    "\n",
    "    phrase_count += 1\n",
    "    \n",
    "# Print total summary length\n",
    "print('Total Summary Length:', len(final_summary), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c35c5-df9b-436f-8126-fcc9c5a335ba",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The summaries produced by TextRank were generally readable and captured the main ideas of the documents. However, because it is extractive, the summaries sometimes feel a bit choppy or repetitive. It also doesn’t understand meaning beyond surface‑level similarity, so it can miss context or over‑emphasize repeated phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d605d0-b266-491b-be69-54de69a59dcd",
   "metadata": {},
   "source": [
    "# Basic Sentence Frequency Summarization\n",
    "\n",
    "<https://stackabuse.com/text-summarization-with-nltk-in-python/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdae4f-c520-48e0-85cf-74efb80b9e66",
   "metadata": {},
   "source": [
    "After TextRank, I implemented a simpler baseline method based on sentence frequency. This approach scores sentences by looking at how often important words appear in them.\n",
    "\n",
    "The process is roughly:\n",
    "\n",
    "- Count word frequencies across the document\n",
    "\n",
    "- Score each sentence based on the frequencies of its words\n",
    "\n",
    "- Select the highest‑scoring sentences for the summary\n",
    "\n",
    "- This method is much simpler than TextRank and doesn’t rely on graphs or similarity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba25396c-3231-47f3-817a-6f80ce79fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81caf4ec-95c2-4a82-b68c-7b2c64d00ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = Counter(Corpus)\n",
    "\n",
    "# Normalize frequencies\n",
    "max_freq = max(word_frequencies.values())\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] /= max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f515e6-64d4-4a7b-b340-e0131f042b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus1 = Corpus[40]\n",
    "sentences = Corpus1.split('.')  # naive sentence split\n",
    "\n",
    "sentence_scores = {}\n",
    "for sent in sentences:\n",
    "    sentence_words = re.findall(r'\\b\\w+\\b', sent.lower())\n",
    "    score = sum(word_frequencies.get(word, 0) for word in sentence_words)\n",
    "    if len(sentence_words) < 30:  # optional length filter\n",
    "        sentence_scores[sent] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3894cb60-cab6-4e5f-b8c8-06d58ead8832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V In this state of schemes, and hopes, and connivance, June opened upon Hartfield.  To Highbury in general it brought no material change.  Elton’s activity in her service, and save herself from being hurried into a delightful situation against her will.  Mr.  Knightley, who, for some reason best known to himself, had certainly taken an early dislike to Frank Churchill, was only growing to dislike him more.\n"
     ]
    }
   ],
   "source": [
    "top_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:5]\n",
    "\n",
    "summary = '. '.join(top_sentences).strip() + '.'\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e65cf6a-1738-458b-bb21-e5f04dc69334",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This method very much surprised me. It tends to favor longer sentences and doesn’t account for redundancy very well. However, the summaries that it produces are very readable and their meaning seems very understandable compared to the choppiness of TextRank. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74a55f-e6dc-4dbb-b5cc-df321e9e947a",
   "metadata": {},
   "source": [
    "## Reflection on Extractive Summarization\n",
    "\n",
    "This is kind of obvious but the extractive summaries just cannot form coherent purpose or meaning since they only take bits out of the input. However, they are surprisingly good at giving meaningful topics and providing the most impactful parts of the text. In many cases, the summaries still give you a solid idea of what the document is about, even if they read a little awkwardly.\n",
    "\n",
    "The main limitation is that extractive methods don’t actually understand the text. They don’t know which sentences depend on each other, and they can’t rewrite or connect ideas in a natural way. This can lead to summaries that repeat information or jump between ideas without smooth transitions.\n",
    "\n",
    "That said, extractive summarization is still useful as a baseline. It’s fast, easy to interpret, and doesn’t require training data. For tasks like topic discovery, quick overviews, or highlighting important sections, these methods work better than expected. This also makes them a good comparison point for more advanced models that try to generate summaries rather than just select sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e579ed4-3caa-4094-b452-db03917b63cb",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "<https://thepythoncode.com/article/text-summarization-using-huggingface-transformers-python>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b6e9e-2e7b-4779-9f97-c9693082bc74",
   "metadata": {},
   "source": [
    "Finally, I explored a transformer‑based summarization approach. Unlike TextRank and frequency methods, transformers use pretrained language models that understand context, grammar, and meaning at a much deeper level.\n",
    "\n",
    "Instead of ranking sentences directly, the transformer model generates summaries based on patterns it learned from large datasets. This allows it to:\n",
    "\n",
    "- Paraphrase instead of just copying sentences\n",
    "\n",
    "- Produce smoother, more natural summaries\n",
    "\n",
    "- Capture high‑level meaning better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1cbbef5-2ea1-4980-8526-1bcc4bf634c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wilsonbeima/Local_Documents/Shenandoah_Work/5_Sem_Fall25/AI/research_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm import tqdm       # progress tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1164d523-0084-40d0-a5ce-070f50f0ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sentence segmentation was performed using a punctuation-based heuristic to avoid external model dependencies.\n",
    "NLTK is the most annoying thing I have ever tried to use.\n",
    "\"\"\"\n",
    "\n",
    "def split_into_sentences(text: str) -> list[str]:\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sentences if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44e1f845-8fd8-4efd-ba0b-36877eface41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_chapter_by_tokens(chapter: str, model_name=\"facebook/bart-large-cnn\", max_tokens=500) -> list[str]:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # tokenizer for counting tokens\n",
    "    sentences = split_into_sentences(chapter)  # split text into sentences\n",
    "    chunks = []\n",
    "    chunk, chunk_len = [], 0  # current chunk and its token count\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = len(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "        if chunk_len + tokens > max_tokens:  # if adding sentence exceeds max_tokens\n",
    "            chunks.append(\" \".join(chunk))  # finish current chunk\n",
    "            chunk, chunk_len = [], 0\n",
    "        chunk.append(sentence)\n",
    "        chunk_len += tokens\n",
    "\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))  # add last chunk\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be5bda92-8340-4e45-b221-b914c49fdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization(summarizer, chapter: str) -> str:\n",
    "    chunks = chunk_chapter_by_tokens(chapter)  # split chapter into token-safe chunks\n",
    "    mini_summaries = []\n",
    "\n",
    "    for chunk in tqdm(chunks, desc=\"Summarizing chunks\"):\n",
    "        max_len = max(30, int(len(chunk.split()) * 0.6))\n",
    "        summary = summarizer(\n",
    "            chunk,\n",
    "            min_length=30,\n",
    "            max_length=max_len,\n",
    "            do_sample=False  # deterministic output\n",
    "        )[0][\"summary_text\"]\n",
    "        mini_summaries.append(summary)\n",
    "\n",
    "    combined_summary = \" \".join(mini_summaries)\n",
    "    max_len_final = max(30, int(len(combined_summary.split()) * 0.6))\n",
    "    \n",
    "    print(\"Generating final summary...\")\n",
    "    final_summary = summarizer(\n",
    "        combined_summary,\n",
    "        min_length=30,\n",
    "        max_length=max_len_final,\n",
    "        do_sample=False\n",
    "    )[0][\"summary_text\"]\n",
    "\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dffd9d76-346c-4462-9266-c057f78d9205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"  # explicitly calls model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53f0dd63-7be4-4075-b502-755d50fa8ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Summarizing chunks: 100%|█████████████████████████████████████████████████████████████████████████████| 9/9 [00:41<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating final summary...\n",
      "Emma Woodhouse was the youngest of the two daughters of a most affectionate, indulgent father. Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses. Sixteen years had Miss Taylor been in Mr. Woodhouse’s family, less as a governess than a friend.\n"
     ]
    }
   ],
   "source": [
    "print(get_summarization(summarizer, Corpus[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4ec37-12be-4760-af6d-850a88724f2a",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "802a8bd6-48b4-4d39-b395-66705c6c7e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Using env variables for API Keys\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc841808-e807-43ce-a3c2-a44a9059e3a5",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "<https://platform.openai.com/docs/quickstart>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d04f2e3f-3847-45aa-9bd9-d61e153da21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c2a9ed7-7d52-4537-96fd-b45ff9183f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e549707-eb07-408a-a2ca-edcd94f3195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_summarizer(text: str, max_tokens: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a given text using ChatGPT.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to summarize.\n",
    "        max_tokens (int): Maximum tokens for the summary.\n",
    "        \n",
    "    Returns:\n",
    "        str: The summary text.\n",
    "    \"\"\"\n",
    "    \n",
    "    client = OpenAI(\n",
    "        # This is the default and can be omitted\n",
    "        api_key=openai_api_key\n",
    "    )\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        instructions=\"You are a helpful assistant that summarizes text concisely.\",\n",
    "        input=f\"Summarize this: {text}\",\n",
    "    )\n",
    "    \n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3ef4c40-8820-4470-aae4-cb0a9ab094c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m chapter_text = Corpus[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m summary = \u001b[43mchatgpt_summarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchapter_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(summary)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mchatgpt_summarizer\u001b[39m\u001b[34m(text, max_tokens)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mSummarizes a given text using ChatGPT.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03m    str: The summary text.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m client = OpenAI(\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# This is the default and can be omitted\u001b[39;00m\n\u001b[32m     15\u001b[39m     api_key=os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a coding assistant that talks like a pirate.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtext\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.output_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Documents/Shenandoah_Work/5_Sem_Fall25/AI/research_project/venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:866\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    829\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    830\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    865\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Documents/Shenandoah_Work/5_Sem_Fall25/AI/research_project/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Local_Documents/Shenandoah_Work/5_Sem_Fall25/AI/research_project/venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "chapter_text = Corpus[0]\n",
    "\n",
    "summary = chatgpt_summarizer(chapter_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ad465-f2f8-4a43-bf73-4b1665ca222f",
   "metadata": {},
   "source": [
    "## Gemini\n",
    "\n",
    "<https://ai.google.dev/gemini-api/docs/text-generation>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5594dc50-04bc-4b52-aac1-1cf5ea3e27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a9cf825-4c45-4fd6-9dab-ea8a8dfdf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\", \"YOUR_GEMINI_API_KEY_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd22ff69-93d3-432c-9cf5-d684afbb1331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_summarizer(text: str, max_tokens: int = 200) -> str:\n",
    "    \"\"\"\n",
    "    Summarizes a given text using ChatGPT.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to summarize.\n",
    "        max_tokens (int): Maximum tokens for the summary.\n",
    "        \n",
    "    Returns:\n",
    "        str: The summary text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "    client = genai.Client(api_key = gemini_api_key)\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"You are a helpful assistant that summarizes text concisely.\"),\n",
    "        contents=f\"Summarize this: {text}\"\n",
    "    )\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64458bdd-c51f-441d-9d78-35db2ebd31d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, a wealthy, clever, and privileged young woman, experiences her first real sorrow when her beloved governess and close companion, Miss Taylor, marries Mr. Weston. This leaves Emma feeling intellectually isolated at Hartfield, as her nervous, change-averse father, Mr. Woodhouse, is a poor conversationalist.\n",
      "\n",
      "While managing her father's distress over the change, Emma takes pride in having orchestrated the match herself four years prior. Mr. Knightley, a sensible family friend, arrives and playfully challenges Emma's claim of \"match-making,\" calling it a \"lucky guess\" and advising against interference. Despite his warnings, Emma immediately decides her next project will be finding a wife for the local vicar, Mr. Elton, prompting further playful admonishments from both her father and Mr. Knightley to let men choose their own spouses.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "chapter_text = Corpus[0]\n",
    "\n",
    "summary = gemini_summarizer(chapter_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de2d90-8020-4dd7-86f1-0cbdc899f3a0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## In this project I messed around with different ways to summarize text. Me tried a few approaches:\n",
    "\n",
    "- Hugging Face Transformers – like BART/T5. Worked okay, but you have to be careful with token limits and chunking or the models freak out on long chapters.\n",
    "\n",
    "- ChatGPT / Google GenAI – way easier for summaries that actually sound like a human wrote them. Less hassle with tokens, but you depend on their APIs.\n",
    "\n",
    "- LDA and other extractive methods – basically tried to pull out the “important sentences” instead of rewriting everything. It worked, but the summaries meren’t as smooth or natural as the LLMs.\n",
    "\n",
    "## What me noticed\n",
    "\n",
    "- Hugging Face is solid if you want control, but handling long texts is annoying.\n",
    "\n",
    "- ChatGPT/GenAI makes everything read nicely and human-like.\n",
    "\n",
    "- LDA/extractive methods are fast and don’t need an API, but can feel robotic or miss context.\n",
    "\n",
    "- Chunking is super important for LLMs, otherwise they crash or cut stuff off.\n",
    "\n",
    "- Progress bars are a lifesaver for big texts.\n",
    "\n",
    "## Overall\n",
    "\n",
    "The LLMs unsurprisingly did the job best for natural-sounding summaries. Hugging Face models were decent, and the extractive stuff like was okay if you just wanted the “important bits” quickly. Overall, all methods gave a decent sense of what the chapters were about, but the LLMs made it way easier to actually read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da77e2-4178-402f-9994-f8baf416ea10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
